:date: 2019-02-04 00:00:00
:title: Webスクレイピング入門の予習 - robots.txt
:category: Study
:tags: Study

==========================================================
Webスクレイピング入門の予習 - robots.txt
==========================================================

2019-02-04

2/11に `PyLadies Tokyo Meetup #38 Webスクレイピング入門 <https://pyladies-tokyo.connpass.com/event/118589/>`_　でスクレイピングのハンズオンの進行をすることになりました。
まぁ、やりたいことは調べながら実装はできるくらいですが、せっかくなので、勉強会駆動学習ということで、なるべく予習していこうと思います。

使うのは以下のライブラリ。

- `Beautiful Soup Documentation — Beautiful Soup 4.4.0 documentation <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>`_
- `Requests: HTTP for Humans™ — Requests 2.21.0 documentation <http://docs.python-requests.org/en/master/>`_

まずは、あまり詳しくないrobots.txtを予習しよう

robots.txt
-----------------
自動でインターネット上に存在するページをどんどん分析していく処理があります。
クローラーといいます。
ほっとくと勝手にどんどんページにアクセスしてきます。

代表格がGoogle様。頼んでもないのに勝手に検索結果にWebサイトを表示してくれます。
まぁ、これがないと便所の落書き、チラ裏のメモと同じなのですが。
他の人が書いた有益な情報が読めるのもGoogle様のおかげです。

そういっても、検索エンジンに出してほしくないページやアクセスしないでほしい場所とかあるでしょう。
ということで、クローラーへの指示を書いておいておくことができます。
これがrobots.txtとrobots metaタグだそうです。

`Robots Exclusion Protocol （Wikipedia） <https://ja.wikipedia.org/wiki/Robots_Exclusion_Standard>`_ として標準化されています。

あくまでクローラーにこうしてね！ってお願いするだけのテキストなので強制力はないそうです。
でも、インターネット上で嫌われたくなかったら守ろうね。って感じみたいです。

試しにこのブログにも置いてみました。

すべてのクローラーに ``_images`` と ``_static`` というフォルダをクロールしないでね。って書いてみました。
::

   User-agent: *
   Disallow: /_images/
   Disallow: /_static/

拘束力はないらしいのですが、検索サイトなどを作るときはRobotsに従ったクロールをする必要があるそうなので、設定しておくのは悪くなさそうですね。
